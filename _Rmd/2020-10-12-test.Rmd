---
 layout: post
 title: "Test"
 author: "Bongseong Kim"
 date: 2020-10-12
 image: /assets/article_images/2020-10-12-chapter1/beginning.jpg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=F, fig.align = "center", message=F, warning=F, fig.height = 8, cache=T, dpi = 300)
set.seed(1)

rm(list = ls())

library(lmerTest)
library(knitr)
library(DT)
library(grf)
if(packageVersion("grf") < '0.10.2') {
  warning("This script requires grf 0.10.2 or higher")
}
library(sandwich)
library(lmtest)
library(Hmisc)
library(ggplot2)

data.all = read.csv("/Users/bong/Library/Mobile Documents/com~apple~CloudDocs/Thesis/Doctor/문헌 고찰/RMD/causal_data.csv")
data.all$schoolid = factor(data.all$schoolid)


DF = data.all[,-1]
school.id = as.numeric(data.all$schoolid)

school.mat = model.matrix(~ schoolid + 0, data = data.all)
school.size = colSums(school.mat)

# It appears that school ID does not affect pscore. So ignore it
# in modeling, and just treat it as source of per-cluster error.
w.lm = glm(Z ~ ., data = data.all[,-3], family = binomial)
summary(w.lm)

W = DF$Z
Y = DF$Y
X.raw = DF[,-(1:2)]

C1.exp = model.matrix(~ factor(X.raw$C1) + 0)
XC.exp = model.matrix(~ factor(X.raw$XC) + 0)

X = cbind(X.raw[,-which(names(X.raw) %in% c("C1", "XC"))], C1.exp, XC.exp)
```
---

#### Data
```{r}
datatable(data.all, extension = "FixedColumns", 
  options = list(
  scrollX = TRUE
))
```
```{r, out.height = "100%", out.width = "100%", fig.align = "center"}
include_graphics("/Users/bong/Library/Mobile Documents/com~apple~CloudDocs/Thesis/Doctor/문헌 고찰/RMD/Covariate.png")
```
C1 변수와 XC 변수는 model.matrix() 함수를 이용하여 가변수 처리함.

---

#### Grow a forest
```{r, echo = T}
( Y.forest = regression_forest(X, Y, clusters = school.id, equalize.cluster.weights = TRUE) )
Y.hat = predict(Y.forest)$predictions
```
```{r, echo = T}
( W.forest = regression_forest(X, W, clusters = school.id, equalize.cluster.weights = TRUE) )
W.hat = predict(W.forest)$predictions
```
W.hat은 Propensity score.
```{r, echo = T}
cf.raw = causal_forest(X, Y, W,
                       Y.hat = Y.hat, W.hat = W.hat,
                       clusters = school.id,
                       equalize.cluster.weights = TRUE)
varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp))
```
전체 covariate을 포함한 RF를 구축한 뒤 variable importance가 전체 평균 보다 높은 변수만 남김.

---

#### Result
"W.hat = W.hat"부분이 Propensity score를 고려하는 부분이며, "clusters = school.id"가 Cluster를 고려하는 부분.
Propensity score를 고려하지 않을 때는 W.hat 대신 mean(W.hat)을 사용.

#### with Propensity score & with Cluster
```{r, echo = T}
( cf = causal_forest(X[,selected.idx], Y, W,
                   Y.hat = Y.hat, W.hat = W.hat,
                   clusters = school.id,
                   equalize.cluster.weights = TRUE,
                   tune.parameters = "all") )
ATE = average_treatment_effect(cf) 
( ATE.yy = paste("95% CI for the ATE:", round(ATE[1], 3),
      "+/-", round(qnorm(0.975) * ATE[2], 3)) )
```

#### with Propensity score & without Cluster
```{r, echo = T}
cf.noclust = causal_forest(X[,selected.idx], Y, W,
                           Y.hat = Y.hat, W.hat = W.hat,
                           tune.parameters = "all")
ATE.noclust = average_treatment_effect(cf.noclust)
( ATE.yn = paste("95% CI for the ATE:", round(ATE.noclust[1], 3),
      "+/-", round(qnorm(0.975) * ATE.noclust[2], 3)) )
```

#### without propensity score / with cluster
```{r, echo = T}
cf.noprop = causal_forest(X[,selected.idx], Y, W,
                          Y.hat = Y.hat, W.hat = mean(W),
                          tune.parameters = "all",
                          equalize.cluster.weights = TRUE,
                          clusters = school.id)
ATE.noprop = average_treatment_effect(cf.noprop)
( ATE.ny = paste("95% CI for the ATE:", round(ATE.noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE.noprop[2], 3)) )
```

#### without propensity score / without cluster
```{r, echo = T}
cf.nono = causal_forest(X[,selected.idx], Y, W,
                          Y.hat = Y.hat, W.hat = mean(W),
                          tune.parameters = "all")
ATE.nono = average_treatment_effect(cf.nono)
( ATE.nn = paste("95% CI for the ATE:", round(ATE.nono[1], 3),
      "+/-", round(qnorm(0.975) * ATE.nono[2], 3)) )
```

#### with Propensity score & with Cluster (Logistic ver.)
```{r, echo = T}
logi = cbind.data.frame(W, X)
logi.res = glm(factor(W) ~ ., data = logi, family = binomial)
W.logi = logi.res$fitted.values
( cf.logi = causal_forest(X[,selected.idx], Y, W,
                   Y.hat = Y.hat, W.hat = W.logi,
                   clusters = school.id,
                   equalize.cluster.weights = TRUE,
                   tune.parameters = "all") )
ATE.logi = average_treatment_effect(cf.logi) 
( ATE.logistic = paste("95% CI for the ATE:", round(ATE.logi[1], 3),
      "+/-", round(qnorm(0.975) * ATE.logi[2], 3)) )
```
Regression forest를 이용하여 Propensity score를 추정할 때 Cluster를 고려하였으나, Logistic은 그렇지 않았다는 차이가 있음.

```{r, fig.align = "center"}
c1 <- rgb(173, 216, 230,max = 255, alpha = 80, names = "lt.blue")
c2 <- rgb(255, 192, 203, max = 255, alpha = 80, names = "lt.pink")
hist(W.logi, breaks = 50, col = c1, xaixs = range(c(W.hat, W.logi)), main = "", xlab = "Propensity score")
hist(W.hat, breaks = 50, col = c2, add = T)
legend("topright", legend = c("Logistic", "RF"), col = c(c1, c2), border = NULL, pch = 15)
```

#### Standard regression model
```{r, echo = T}
reg <- cbind.data.frame(Y, W, X[,selected.idx])
lm.result <- lm(Y ~ W + ., data = reg)
out <- summary(lm.result)
out$coefficients["W", 1:2]
( ATE.lm = paste("95% CI for the ATE:", round(out$coefficients["W",1], 3),
                       "+/-", round(out$coefficients["W", 2], 3)) )
```

#### Mixed regression model - 절편, 기울기
```{r, echo = T}
mix <- cbind.data.frame(school.id, Y, W, X[,selected.idx])
mix.result <- lmer(Y ~ W + (1 + W | school.id) + ., data = mix)
out <- summary(mix.result)
out$coefficients["W", 1:2]
( ATE.mix = paste("95% CI for the ATE:", round(out$coefficients["W",1], 3),
                 "+/-", round(out$coefficients["W", 2], 3)) )
```

---

#### Summary
CRF, with ps / with cluster :  
`r ATE.yy`  

CRF, with ps / without cluster :  
`r ATE.yn`  

CRF, without ps / with cluster :  
`r ATE.ny`  

CRF, without ps / without cluster :  
`r ATE.nn`  

CRF, using logistic ps / with cluster :  
`r ATE.logistic`  

Standard regression :  
`r ATE.lm`  

Mixed regression, 절편 + 기울기 :  
`r ATE.mix`  
